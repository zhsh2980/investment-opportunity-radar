"""
æŠ•èµ„æœºä¼šé›·è¾¾ - Slot ä»»åŠ¡

æ¯ä¸ª Slot çš„æ€»ç¼–æ’é€»è¾‘ï¼š
1. åˆ›å»º/è·å– slot_runï¼ˆå¹‚ç­‰ï¼‰
2. ä» WeRSS æ‹‰å–å¹¶å…¥åº“æ–‡ç« 
3. é€ç¯‡åˆ†æ
4. æ¨é€æœºä¼š/æ—¥æŠ¥
"""
import json
import hashlib
from datetime import datetime, timedelta, date
from typing import Optional, List

from celery import shared_task
from sqlalchemy import and_

from ..database import SessionLocal
from ..domain.models import (
    SlotRun,
    ContentItem,
    AnalysisResult,
    DailyReport,
    PromptVersion,
    NotificationLog,
)
from ..services.analyzer import (
    fetch_and_save_articles,
    analyze_article,
    get_active_prompt,
    get_setting_value,
    should_push_opportunity,
    push_opportunity_alert,
    generate_msg_uuid,
    has_content,
    try_refresh_content,
)
from ..clients.dingtalk import get_dingtalk_client
from ..clients.deepseek import get_deepseek_client
from ..core.prompts import DAILY_DIGEST_SYSTEM_PROMPT, DAILY_DIGEST_USER_TEMPLATE
from ..config import get_settings
from ..logging_config import get_logger

logger = get_logger(__name__)


def get_or_create_slot_run(session, run_date: date, slot: str) -> tuple[SlotRun, bool]:
    """
    è·å–æˆ–åˆ›å»º slot_runï¼ˆå¹‚ç­‰ï¼‰
    
    Returns:
        (slot_run, is_new)
    """
    existing = session.query(SlotRun).filter(
        SlotRun.run_date == run_date,
        SlotRun.slot == slot,
    ).first()
    
    if existing:
        return existing, False
    
    # è®¡ç®—æ—¶é—´çª—å£
    now = datetime.now()
    window_days = get_setting_value(session, "window_days", 3)
    window_end = now
    window_start = now - timedelta(days=window_days)
    
    slot_run = SlotRun(
        run_date=run_date,
        slot=slot,
        window_start_at=window_start,
        window_end_at=window_end,
        status=0,  # è¿›è¡Œä¸­
        stats={},
    )
    session.add(slot_run)
    session.commit()
    
    return slot_run, True


def is_last_slot_of_day(session, current_slot: str) -> bool:
    """
    åˆ¤æ–­å½“å‰ slot æ˜¯å¦ä¸ºå½“å¤©æœ€åä¸€ä¸ªæ—¶é—´ç‚¹
    
    ä»æ•°æ®åº“è¯»å– schedule_slots é…ç½®ï¼ŒæŒ‰æ—¶é—´æ’åºååˆ¤æ–­
    """
    from ..domain.models import Settings
    
    # é»˜è®¤å€¼
    default_slots = ["07:00", "12:00", "14:00", "18:00", "22:00"]
    
    # ä»æ•°æ®åº“è¯»å–
    setting = session.query(Settings).filter(Settings.key == "schedule_slots").first()
    if setting and setting.value_json:
        slots = setting.value_json
    else:
        slots = default_slots
    
    # æŒ‰æ—¶é—´æ’åº
    sorted_slots = sorted(slots)
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€ä¸ª
    if sorted_slots:
        return current_slot == sorted_slots[-1]
    return False


@shared_task(bind=True, max_retries=2, default_retry_delay=60)
def run_slot(self, slot: str, manual: bool = False):
    """
    æ‰§è¡Œä¸€ä¸ª slot çš„å®Œæ•´æµç¨‹ (Celery Task Wrapper)
    """
    execute_slot(slot, manual)


def execute_slot(slot: str, manual: bool = False):
    """
    æ‰§è¡Œä¸€ä¸ª slot çš„å®Œæ•´æµç¨‹ (Core Logic)
    
    Args:
        slot: æ—¶æ®µæ ‡è¯†ï¼Œå¦‚ "07:00", "12:00", "22:00"
        manual: æ˜¯å¦ä¸ºæ‰‹åŠ¨è§¦å‘
    """
    settings = get_settings()
    session = SessionLocal()
    
    try:
        # è·å–å½“å‰æ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
        now = datetime.now()
        run_date = now.date()
        
        logger.info(f"===== å¼€å§‹æ‰§è¡Œ slot: {run_date} {slot} =====")
        
        # 1. è·å–æˆ–åˆ›å»º slot_runï¼ˆå¹‚ç­‰ï¼‰
        slot_run, is_new = get_or_create_slot_run(session, run_date, slot)
        
        if not is_new and slot_run.status == 1:
            logger.info(f"slot_run å·²æˆåŠŸå®Œæˆï¼Œè·³è¿‡: {run_date} {slot}")
            return {"status": "skipped", "reason": "already_completed"}
        
        if not is_new and slot_run.status == 0:
            logger.warning(f"slot_run æ­£åœ¨è¿è¡Œä¸­ï¼ˆå¯èƒ½é‡å¤è§¦å‘ï¼‰: {run_date} {slot}")
            # å…è®¸ç»§ç»­ï¼ˆå¯èƒ½æ˜¯é‡å¯åæ¢å¤ï¼‰
        
        slot_run.status = 0  # è¿›è¡Œä¸­
        slot_run.started_at = datetime.utcnow()
        session.commit()
        
        stats = {
            "articles_fetched": 0,
            "articles_new": 0,
            "articles_analyzed": 0,
            "articles_failed": 0,
            "opportunities_found": 0,
            "pushed": False,
        }
        
        try:
            # 2. ä» WeRSS æ‹‰å–å¹¶å…¥åº“æ–‡ç« 
            logger.info("å¼€å§‹è·å–æ–‡ç« ...")
            new_items = fetch_and_save_articles(
                session=session,
                start_time=slot_run.window_start_at,
                end_time=slot_run.window_end_at,
            )
            stats["articles_new"] = len(new_items)
            logger.info(f"æ–°å¢æ–‡ç« : {len(new_items)} ç¯‡")
            
            # 3. è·å–å¾…åˆ†ææ–‡ç« 
            pending_items = session.query(ContentItem).filter(
                ContentItem.analyzed_status == 0,
                ContentItem.published_at >= slot_run.window_start_at,
            ).all()
            
            stats["articles_fetched"] = len(pending_items)
            stats["articles_total"] = len(pending_items)  # åˆå§‹æ€»æ•°ï¼Œç”¨äºè¿›åº¦æ˜¾ç¤º
            # ç«‹å³æ›´æ–° stats åˆ°æ•°æ®åº“ï¼Œè®©è¿›åº¦ API èƒ½è¯»å–
            slot_run.stats = stats.copy()
            session.commit()
            logger.info(f"å¾…åˆ†ææ–‡ç« : {len(pending_items)} ç¯‡")
            
            # è·å–æ´»è·ƒçš„ Prompt
            prompt_version = get_active_prompt(session)
            if not prompt_version:
                logger.warning("æœªæ‰¾åˆ°æ´»è·ƒçš„ Promptï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºå½“å¤©æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆåŠ¨æ€è·å–ï¼‰
            is_last_slot = is_last_slot_of_day(session, slot)
            base_url = f"http://154.8.205.159:8080"  # TODO: ä»é…ç½®è¯»å–
            
            # 4. é€ç¯‡åˆ†æ + ç«‹å³æ¨é€
            pushed_count = 0
            skipped_count = 0  # è·³è¿‡çš„æ— æ­£æ–‡æ–‡ç« æ•°
            for item in pending_items:
                try:
                    # æ£€æŸ¥æ­£æ–‡ï¼Œæ— æ­£æ–‡åˆ™å°è¯•é‡æ–°æ‹‰å–
                    if not has_content(item):
                        item = try_refresh_content(session, item)
                        if not has_content(item):
                            logger.info(f"è·³è¿‡æ— æ­£æ–‡æ–‡ç« : {item.title[:30]}")
                            skipped_count += 1
                            continue  # è·³è¿‡ï¼Œä¸æ ‡è®°å·²åˆ†æ
                    
                    analysis = analyze_article(
                        session=session,
                        content_item=item,
                        prompt_version=prompt_version,
                        run_id=slot_run.id,
                    )
                    if analysis:
                        stats["articles_analyzed"] += 1
                        if analysis.has_opportunity:
                            stats["opportunities_found"] += 1
                            # ç«‹å³æ¨é€æœ‰æœºä¼šçš„æ–‡ç« 
                            if should_push_opportunity(session, analysis, str(run_date), slot):
                                pushed = push_opportunity_alert(
                                    session=session,
                                    analysis=analysis,
                                    run_date=str(run_date),
                                    slot=slot,
                                    base_url=base_url,
                                )
                                if pushed:
                                    pushed_count += 1
                                    logger.info(f"å·²æ¨é€æœºä¼š: {item.title[:30]}, score={analysis.score}")
                    else:
                        stats["articles_failed"] += 1
                except Exception as e:
                    logger.error(f"åˆ†ææ–‡ç« å¤±è´¥: {item.title[:30]}, {e}")
                    stats["articles_failed"] += 1
            
            stats["articles_skipped"] = skipped_count
            if skipped_count > 0:
                logger.info(f"è·³è¿‡æ— æ­£æ–‡æ–‡ç« : {skipped_count} ç¯‡")
            
            # 5. æ— æœºä¼šæ—¶çš„é€šçŸ¥ + æ—¥æŠ¥
            if is_last_slot:
                # æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ï¼šæ— æœºä¼šæ—¶å‘é€"å½“å¤©æ²¡æœ‰æœºä¼š"é€šçŸ¥
                # æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ï¼šæ— è®ºæœ‰æ— æœºä¼šï¼Œéƒ½å¿…å‘æ—¥æŠ¥
                # (ç§»é™¤ push_no_opportunity_todayï¼Œé¿å…é‡å¤é€šçŸ¥)
                # å¿…å‘æ—¥æŠ¥ï¼ˆæ— è®ºæœ‰æ— æœºä¼šï¼‰
                success = generate_and_push_daily_report(
                    session=session,
                    run_date=run_date,
                    slot=slot,
                    base_url=base_url,
                )
                stats["pushed"] = success
            elif manual:
                # æ‰‹åŠ¨æ¨¡å¼ï¼šæ— æœºä¼šæ—¶å‘é€æ±‡æ€»é€šçŸ¥
                if pushed_count == 0:
                    from ..services.analyzer import push_manual_summary
                    push_manual_summary(
                        session=session,
                        analyzed_count=stats["articles_analyzed"],
                        run_date=str(run_date),
                        slot=slot,
                    )
                stats["pushed"] = pushed_count > 0 or True  # æ‰‹åŠ¨æ¨¡å¼æ€»æ˜¯æ ‡è®°ä¸ºå·²æ¨é€
            else:
                # éæœ€åä¸€æ¬¡å®šæ—¶ï¼šæ— æœºä¼šä¸å‘ä»»ä½•é€šçŸ¥
                stats["pushed"] = pushed_count > 0
            
            stats["pushed_count"] = pushed_count
            
            # 6. æ›´æ–° slot_run çŠ¶æ€
            slot_run.status = 1  # æˆåŠŸ
            slot_run.stats = stats
            slot_run.finished_at = datetime.utcnow()
            session.commit()
            
            logger.info(f"===== slot å®Œæˆ: {run_date} {slot}, stats={stats} =====")
            return {"status": "success", "stats": stats}
            
        except Exception as e:
            logger.error(f"slot æ‰§è¡Œå¤±è´¥: {e}")
            slot_run.status = 2  # å¤±è´¥
            slot_run.error = str(e)
            slot_run.finished_at = datetime.utcnow()
            session.commit()
            raise
            
    finally:
        session.close()


def generate_and_push_daily_report(
    session,
    run_date: date,
    slot: str,
    base_url: str,
) -> bool:
    """
    ç”Ÿæˆå¹¶æ¨é€æ—¥æŠ¥ï¼ˆ22:00 å¿…æ¨ï¼‰
    """
    logger.info(f"å¼€å§‹ç”Ÿæˆæ—¥æŠ¥: {run_date}")
    
    # è·å–å½“å¤©æ‰€æœ‰åˆ†æç»“æœ
    today_analyses = session.query(AnalysisResult).join(ContentItem).filter(
        and_(
            ContentItem.published_at >= datetime.combine(run_date, datetime.min.time()),
            ContentItem.published_at < datetime.combine(run_date + timedelta(days=1), datetime.min.time()),
        )
    ).all()
    
    # æ„å»º compact ç»“æ„
    analyses_compact = []
    for a in today_analyses:
        item = a.content_item
        opp_types = a.result_json.get("opportunity_types", [])
        analyses_compact.append({
            "title": item.title,
            "mp_name": item.mp_name,
            "published_at": item.published_at.isoformat(),
            "score": a.score,
            "has_opportunity": a.has_opportunity,
            "top_type": opp_types[0] if opp_types else "",
            "summary": a.summary_md[:200],
            "analysis_url": f"{base_url}/analysis/{a.id}",
        })
    
    # è·å–é˜ˆå€¼
    threshold = get_setting_value(session, "push_score_threshold", 60)
    
    # ç»Ÿè®¡
    total_articles = len(analyses_compact)
    opportunities = [a for a in analyses_compact if a["has_opportunity"] and a["score"] >= threshold]
    total_opportunities = len(opportunities)
    
    # ç”Ÿæˆæ—¥æŠ¥å†…å®¹ï¼ˆè°ƒç”¨ DeepSeek æˆ–ä½¿ç”¨ç®€å•æ¨¡æ¿ï¼‰
    if total_articles > 0:
        try:
            digest_result = generate_daily_digest(
                date=str(run_date),
                threshold=threshold,
                analyses=analyses_compact,
            )
            digest_md = digest_result.get("digest_md", "")
            has_opportunity = digest_result.get("has_opportunity", False)
        except Exception as e:
            logger.error(f"æ—¥æŠ¥ AI ç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šç”Ÿæˆç®€å•åˆ—è¡¨
            digest_md = f"## {run_date} æ—¥æŠ¥ (AI ç”Ÿæˆé‡åˆ°é—®é¢˜)\n\n"
            digest_md += f"**ç»Ÿè®¡**: å…±åˆ†æ {total_articles} ç¯‡æ–‡ç« ï¼Œå‘ç° {total_opportunities} ä¸ªæœºä¼š\n\n"
            digest_md += "### æ–‡ç« åˆ—è¡¨\n"
            for a in analyses_compact:
                icon = "ğŸ¯" if a["has_opportunity"] and a["score"] >= threshold else "ğŸ“„"
                digest_md += f"- {icon} [{a['title']}]({a['analysis_url']}) ({a['score']}åˆ†)\n"
            
            # æ‰‹åŠ¨è®¡ç®— has_opportunity
            has_opportunity = total_opportunities > 0
    else:
        digest_md = f"## {run_date} æ—¥æŠ¥\n\n**ä»Šæ—¥æ— æ–°æ–‡ç« åˆ†æã€‚**"
        has_opportunity = False
    
    # ä¿å­˜æ—¥æŠ¥
    report = session.query(DailyReport).filter(DailyReport.report_date == run_date).first()
    if report:
        report.digest_md = digest_md
        report.digest_json = {"analyses": analyses_compact}
        report.stats = {
            "articles_analyzed": total_articles,
            "opportunities_found": total_opportunities,
        }
        report.updated_at = datetime.utcnow()
    else:
        report = DailyReport(
            report_date=run_date,
            digest_md=digest_md,
            digest_json={"analyses": analyses_compact},
            slots_done=[slot],
            stats={
                "articles_analyzed": total_articles,
                "opportunities_found": total_opportunities,
            },
        )
        session.add(report)
    session.commit()
    
    # æ¨é€æ—¥æŠ¥
    dingtalk = get_dingtalk_client()
    msg_uuid = generate_msg_uuid(str(run_date), slot, "daily")
    
    try:
        result = dingtalk.send_daily_report(
            date=str(run_date),
            has_opportunity=has_opportunity,
            total_articles=total_articles,
            total_opportunities=total_opportunities,
            digest=digest_md[:500],  # æ‘˜è¦
            base_url=base_url,
            msg_uuid=msg_uuid,
        )
        
        success = result.get("errcode") == 0
        
        # è®°å½•æ¨é€æ—¥å¿—
        log = NotificationLog(
            report_date=str(run_date),
            slot=slot,
            push_type="daily",
            msg_uuid=msg_uuid,
            target_url=f"{base_url}/daily/{run_date}",
            title=f"æ—¥æŠ¥ {run_date}",
            payload={"report_date": str(run_date)},
            response=result,
            status=1 if success else 2,
            error=result.get("errmsg") if not success else None,
        )
        session.add(log)
        session.commit()
        
        logger.info(f"æ—¥æŠ¥æ¨é€{'æˆåŠŸ' if success else 'å¤±è´¥'}: {run_date}")
        return success
        
    except Exception as e:
        logger.error(f"æ—¥æŠ¥æ¨é€å¼‚å¸¸: {e}")
        return False


def generate_daily_digest(date: str, threshold: int, analyses: list) -> dict:
    """
    ä½¿ç”¨ DeepSeek ç”Ÿæˆæ—¥æŠ¥å†…å®¹
    """
    deepseek = get_deepseek_client()
    
    user_prompt = DAILY_DIGEST_USER_TEMPLATE.format(
        date=date,
        threshold=threshold,
        today_analyses_json=json.dumps(analyses, ensure_ascii=False, indent=2),
    )
    
    result = deepseek.analyze_article(
        system_prompt=DAILY_DIGEST_SYSTEM_PROMPT,
        article_content=user_prompt,
    )
    
    return result
